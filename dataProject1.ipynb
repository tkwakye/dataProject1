{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.6.17.tar.gz (82 kB)\n",
      "     ---------------------------------------- 82.7/82.7 kB 4.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\tiffa\\appdata\\roaming\\python\\python311\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in c:\\users\\tiffa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kaggle) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\tiffa\\appdata\\roaming\\python\\python311\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in c:\\users\\tiffa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tiffa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kaggle) (4.66.2)\n",
      "Collecting python-slugify\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\tiffa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kaggle) (2.2.3)\n",
      "Collecting bleach\n",
      "  Downloading bleach-6.1.0-py3-none-any.whl (162 kB)\n",
      "     ------------------------------------- 162.8/162.8 kB 10.2 MB/s eta 0:00:00\n",
      "Collecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.2/78.2 kB 4.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tiffa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tiffa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->kaggle) (3.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\tiffa\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.6.17-py3-none-any.whl size=105795 sha256=f9f1ba9f2cb0a67b22189f2c87ee9a4da691a955c2ddd4f18dfc518f05db1802\n",
      "  Stored in directory: c:\\users\\tiffa\\appdata\\local\\pip\\cache\\wheels\\a0\\1a\\35\\76ccd43f313c123bbbf2d21b44d714b14098ad6271f2e08dca\n",
      "Successfully built kaggle\n",
      "Installing collected packages: webencodings, text-unidecode, python-slugify, bleach, kaggle\n",
      "Successfully installed bleach-6.1.0 kaggle-1.6.17 python-slugify-8.0.4 text-unidecode-1.3 webencodings-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymysql\n",
      "  Downloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
      "     ---------------------------------------- 45.0/45.0 kB 2.2 MB/s eta 0:00:00\n",
      "Installing collected packages: pymysql\n",
      "Successfully installed pymysql-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pymysql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SQLAlchemy\n",
      "  Downloading SQLAlchemy-2.0.36-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 22.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\tiffa\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from SQLAlchemy) (4.12.2)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-3.1.1-cp311-cp311-win_amd64.whl (298 kB)\n",
      "     ------------------------------------- 298.9/298.9 kB 18.0 MB/s eta 0:00:00\n",
      "Installing collected packages: greenlet, SQLAlchemy\n",
      "Successfully installed SQLAlchemy-2.0.36 greenlet-3.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install SQLAlchemy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = \"tiffaniek\"\n",
    "os.environ['KAGGLE_KEY'] = \"eacd0490db35bd16fd2d95b447878d4c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/umeradnaan/prediction-of-disaster-management-in-2024\n",
      "License(s): CC0-1.0\n",
      "Downloading prediction-of-disaster-management-in-2024.zip to c:\\Users\\tiffa\\OneDrive\\Desktop\\DataScienceSystems\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/293k [00:00<?, ?B/s]\n",
      "100%|██████████| 293k/293k [00:00<00:00, 10.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = r\"C:\\Users\\tiffa\\OneDrive\\Desktop\\DataScienceSystems\\kaggle.json\"\n",
    "\n",
    "!kaggle datasets download -d umeradnaan/prediction-of-disaster-management-in-2024\n",
    "\n",
    "\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('prediction-of-disaster-management-in-2024.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(r'C:\\Users\\tiffa\\OneDrive\\Desktop\\DataScienceSystems')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/26.5M [00:00<?, ?B/s]\n",
      "  4%|▍         | 1.00M/26.5M [00:00<00:03, 7.10MB/s]\n",
      "  8%|▊         | 2.00M/26.5M [00:00<00:03, 7.26MB/s]\n",
      " 11%|█▏        | 3.00M/26.5M [00:00<00:03, 7.08MB/s]\n",
      " 15%|█▌        | 4.00M/26.5M [00:00<00:03, 7.29MB/s]\n",
      " 19%|█▉        | 5.00M/26.5M [00:00<00:03, 6.82MB/s]\n",
      " 23%|██▎       | 6.00M/26.5M [00:00<00:03, 6.89MB/s]\n",
      " 26%|██▋       | 7.00M/26.5M [00:01<00:02, 7.14MB/s]\n",
      " 30%|███       | 8.00M/26.5M [00:01<00:02, 7.39MB/s]\n",
      " 34%|███▍      | 9.00M/26.5M [00:01<00:02, 7.31MB/s]\n",
      " 38%|███▊      | 10.0M/26.5M [00:01<00:02, 7.49MB/s]\n",
      " 41%|████▏     | 11.0M/26.5M [00:01<00:02, 7.45MB/s]\n",
      " 45%|████▌     | 12.0M/26.5M [00:01<00:02, 7.49MB/s]\n",
      " 49%|████▉     | 13.0M/26.5M [00:01<00:01, 7.47MB/s]\n",
      " 53%|█████▎    | 14.0M/26.5M [00:02<00:01, 7.21MB/s]\n",
      " 57%|█████▋    | 15.0M/26.5M [00:02<00:01, 7.24MB/s]\n",
      " 60%|██████    | 16.0M/26.5M [00:02<00:01, 7.42MB/s]\n",
      " 64%|██████▍   | 17.0M/26.5M [00:02<00:01, 7.44MB/s]\n",
      " 68%|██████▊   | 18.0M/26.5M [00:02<00:01, 7.45MB/s]\n",
      " 72%|███████▏  | 19.0M/26.5M [00:02<00:01, 7.75MB/s]\n",
      " 75%|███████▌  | 20.0M/26.5M [00:02<00:00, 7.43MB/s]\n",
      " 79%|███████▉  | 21.0M/26.5M [00:02<00:00, 7.52MB/s]\n",
      " 83%|████████▎ | 22.0M/26.5M [00:03<00:00, 7.43MB/s]\n",
      " 87%|████████▋ | 23.0M/26.5M [00:03<00:00, 7.44MB/s]\n",
      " 90%|█████████ | 24.0M/26.5M [00:03<00:00, 7.65MB/s]\n",
      " 94%|█████████▍| 25.0M/26.5M [00:03<00:00, 7.61MB/s]\n",
      " 98%|█████████▊| 26.0M/26.5M [00:03<00:00, 7.64MB/s]\n",
      "100%|██████████| 26.5M/26.5M [00:03<00:00, 7.42MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/rmisra/news-category-dataset\n",
      "License(s): Attribution 4.0 International (CC BY 4.0)\n",
      "Downloading news-category-dataset.zip to c:\\Users\\tiffa\\OneDrive\\Desktop\\DataScienceSystems\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!kaggle datasets download -d rmisra/news-category-dataset\n",
    "\n",
    "\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('news-category-dataset.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(r'C:\\Users\\tiffa\\OneDrive\\Desktop\\DataScienceSystems')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a dataset:\n",
      "1. News Category Dataset\n",
      "2. Prediction of Disaster Management in 2024\n",
      "\n",
      "Select the desired output format:\n",
      "1. CSV\n",
      "2. JSON\n",
      "3. SQL\n",
      "Number of records: 10000\n",
      "Number of columns: 7\n",
      "Data successfully written to nnt6qd database.\n",
      "      Disaster_ID Disaster_Type   Location  Magnitude                 Date  \\\n",
      "0               1      Wildfire     Brazil   6.267393  2024-01-01 00:00:00   \n",
      "1               2     Hurricane  Indonesia   6.649358  2024-01-01 01:00:00   \n",
      "2               3       Tornado      China   9.724366  2024-01-01 02:00:00   \n",
      "3               4         Flood      India   1.702505  2024-01-01 03:00:00   \n",
      "4               5         Flood     Brazil   7.917748  2024-01-01 04:00:00   \n",
      "...           ...           ...        ...        ...                  ...   \n",
      "9995         9996     Hurricane      China   3.092801  2025-02-20 11:00:00   \n",
      "9996         9997       Tornado     Brazil   6.652682  2025-02-20 12:00:00   \n",
      "9997         9998    Earthquake  Indonesia   6.824656  2025-02-20 13:00:00   \n",
      "9998         9999         Flood        USA   6.381571  2025-02-20 14:00:00   \n",
      "9999        10000    Earthquake     Brazil   8.556928  2025-02-20 15:00:00   \n",
      "\n",
      "      Fatalities  Economic_Loss($)  \n",
      "0           9706      6.509790e+08  \n",
      "1           2233      5.538357e+08  \n",
      "2            478      6.910998e+07  \n",
      "3           2867      8.474880e+08  \n",
      "4            776      6.449297e+08  \n",
      "...          ...               ...  \n",
      "9995        7837      1.067089e+07  \n",
      "9996        6288      1.414537e+08  \n",
      "9997         673      8.330828e+08  \n",
      "9998         323      9.502971e+08  \n",
      "9999        2620      3.732303e+08  \n",
      "\n",
      "[10000 rows x 7 columns]\n",
      "Data stored in MySQL table 'natural_disasters_2024' in database 'nnt6qd' after dropping 'Disaster_ID' column.\n",
      "Number of records: 10000\n",
      "Number of columns: 6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "import json\n",
    "\n",
    "# Database connection details for phpMyAdmin\n",
    "host_name = \"ds2002.org\" \n",
    "host_ip = \"ds2002.org\"\n",
    "port = \"3306\"\n",
    "\n",
    "user_id = \"nnt6qd\"\n",
    "pwd = \"nnt6qd!\"\n",
    "db_name = \"nnt6qd\"\n",
    "\n",
    "# Function to drop the first column from the dataset and save it to a table in my database\n",
    "def drop_column_and_save_to_mysql(df, table_name, db_name):\n",
    "\n",
    "    column_to_drop = df.columns[0]\n",
    "    df = df.drop(columns=[column_to_drop])\n",
    "\n",
    "    engine = create_engine(f\"mysql+pymysql://{user_id}:{pwd}@{host_name}/{db_name}\")\n",
    "\n",
    "    df.to_sql(table_name, engine, if_exists='replace', index=False)\n",
    "    \n",
    "    print(f\"Data stored in MySQL table '{table_name}' in database '{db_name}' after dropping '{column_to_drop}' column.\")\n",
    "    print(f\"Number of records: {len(df)}\")\n",
    "    print(f\"Number of columns: {df.shape[1]}\")\n",
    "\n",
    "# Function to load a dataset (either CSV or JSON), drop a column, and save it to a table in my database\n",
    "def ingest_and_modify_data(file_path, file_format):\n",
    "    if file_format == 'csv':\n",
    "        df = pd.read_csv(file_path)\n",
    "        table_name = file_path.replace('.csv', '')\n",
    "        drop_column_and_save_to_mysql(df, table_name, db_name)\n",
    "    elif file_format == 'json':\n",
    "        df = pd.read_json(file_path, lines=True)\n",
    "        table_name = file_path.replace('.json', '')\n",
    "        drop_column_and_save_to_mysql(df, table_name, db_name)\n",
    "\n",
    "\n",
    "# Function to download the dataset based on user choice\n",
    "def download_dataset(dataset_choice):\n",
    "    if dataset_choice == \"news-category\":\n",
    "        os.system('kaggle datasets download -d rmisra/news-category-dataset')\n",
    "        dataset_path = 'News_Category_Dataset_v3.json'\n",
    "        return dataset_path\n",
    "    elif dataset_choice == \"natural-disaster\":\n",
    "        dataset_path = 'natural_disasters_2024.csv'\n",
    "        return dataset_path\n",
    "\n",
    "# Convert CSV to JSON \n",
    "def convert_to_json(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"Number of records: {len(df)}\")\n",
    "    print(f\"Number of columns: {df.shape[1]}\")\n",
    "    df = df.dropna()\n",
    "    json_file = csv_file.replace('.csv', '.json')\n",
    "    df.to_json(json_file, orient='records', lines=False)\n",
    "    print(f\"Converted to JSON: {json_file}\")\n",
    "    with open(json_file, 'r') as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "            print(json.dumps(data, indent=4)) \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to decode JSON: {e}\")\n",
    "\n",
    "# Convert JSON to CSV\n",
    "def convert_to_csv(json_file):\n",
    "    data = []\n",
    "    with open(json_file) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Number of records: {len(df)}\")\n",
    "    print(f\"Number of columns: {df.shape[1]}\")\n",
    "    csv_file = json_file.replace('.json', '.csv')\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    print(f\"Data successfully written to {csv_file}\")\n",
    "    print(\"\\nCSV content:\")\n",
    "    print(df)\n",
    "\n",
    "# Function to convert a CSV file to a SQL table\n",
    "def convert_to_sql(csv_file, db_name, host_name, user_id, pwd):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"Number of records: {len(df)}\")\n",
    "    print(f\"Number of columns: {df.shape[1]}\")\n",
    "    engine = create_engine(f\"mysql+pymysql://{user_id}:{pwd}@{host_name}/{db_name}\")\n",
    "\n",
    "    df.to_sql('ELT Table', con=engine, if_exists='replace', index=False)\n",
    "    \n",
    "    print(f\"Data successfully written to {db_name} database.\")\n",
    "    \n",
    "    query = \"SELECT * FROM data\"\n",
    "    df_sql = pd.read_sql(query, con=engine)\n",
    "    \n",
    "    print(df_sql)\n",
    "\n",
    "# Main function to prompt user for input and process the selected dataset\n",
    "def main():\n",
    "    host_name = \"ds2002.org\" \n",
    "    host_ip = \"ds2002.org\"\n",
    "    port = \"3306\"\n",
    "\n",
    "    user_id = \"nnt6qd\"\n",
    "    pwd = \"nnt6qd!\"\n",
    "    db_name = \"nnt6qd\"\n",
    "\n",
    "    # Prompt the user to select a dataset\n",
    "    print(\"Select a dataset:\")\n",
    "    print(\"1. News Category Dataset\")\n",
    "    print(\"2. Prediction of Disaster Management in 2024\")\n",
    "    dataset_choice = input(\"Enter 1 or 2: \").strip()\n",
    "\n",
    "    if dataset_choice == '1':\n",
    "        dataset_path = download_dataset(\"news-category\")\n",
    "    elif dataset_choice == '2':\n",
    "        dataset_path = download_dataset(\"natural-disaster\")\n",
    "    else:\n",
    "        print(\"Invalid selection.\")\n",
    "        return\n",
    "\n",
    "    # Prompt the user to select an output format\n",
    "    print(\"\\nSelect the desired output format:\")\n",
    "    print(\"1. CSV\")\n",
    "    print(\"2. JSON\")\n",
    "    print(\"3. SQL\")\n",
    "    format_choice = input(\"Enter 1, 2, or 3: \").strip()\n",
    "\n",
    "\n",
    "    if format_choice == '1': \n",
    "        if dataset_path.endswith('.csv'):\n",
    "            print(\"The dataset is already in CSV format.\")\n",
    "            df = pd.read_csv(dataset_path)\n",
    "            print(f\"Number of records: {len(df)}\")\n",
    "            print(f\"Number of columns: {df.shape[1]}\")\n",
    "        elif dataset_path.endswith('.json'):\n",
    "            convert_to_csv(dataset_path)\n",
    "\n",
    "    elif format_choice == '2':\n",
    "        if dataset_path.endswith('.json'):\n",
    "            print(\"The dataset is already in JSON format.\")\n",
    "            data = []\n",
    "            with open(dataset_path) as f:\n",
    "                for line in f:\n",
    "                    data.append(json.loads(line))\n",
    "            df = pd.DataFrame(data)\n",
    "            print(f\"Number of records: {len(df)}\")\n",
    "            print(f\"Number of columns: {df.shape[1]}\")\n",
    "        elif dataset_path.endswith('.csv'):\n",
    "            convert_to_json(dataset_path)\n",
    "\n",
    "    elif format_choice == '3':\n",
    "        if dataset_path.endswith('.csv'):\n",
    "             convert_to_sql(dataset_path, db_name, host_name, user_id, pwd)\n",
    "        else:\n",
    "            print(\"SQL conversion only works with CSV files.\")\n",
    "            data = []\n",
    "            with open(dataset_path) as f:\n",
    "                for line in f:\n",
    "                    data.append(json.loads(line))\n",
    "            df = pd.DataFrame(data)\n",
    "            print(f\"Number of records: {len(df)}\")\n",
    "            print(f\"Number of columns: {df.shape[1]}\")\n",
    "    else:\n",
    "        print(\"Invalid selection.\")\n",
    "    \n",
    "    # Determine file format and process the data\n",
    "    if dataset_path.endswith('.csv'):\n",
    "        file_format = 'csv'\n",
    "    elif dataset_path.endswith('.json'):\n",
    "        file_format = 'json'\n",
    "    else:\n",
    "        print(\"Unsupported file format\")\n",
    "        return\n",
    "    ingest_and_modify_data(dataset_path, file_format)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
